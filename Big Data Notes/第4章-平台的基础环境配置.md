## 实验一：基础环境配置

### Master节点
查看IP地址
```bash
ip a
```
修改主机名为master
```bash
hostnamectl set-hostname master
```
```bash
bash
```
修改/etc/hosts文件，添加主机映射
```bash
vim /etc/hosts
```
添加如下内容
```
192.168.66.114 master
192.168.66.115 slave1
192.168.66.116 slave2
```
修改配置文件
```bash
vim /etc/sysconfig/ntpd
```
添加如下内容
```
YS_HWLOCK=yes
```
同步时间
```bash
systemctl start ntpd
```
```bash
date
```
关闭防火墙
```bash
systemctl stop firewalld.service
```
关闭防火墙自启
```bash
systemctl disable firewalld.service
```
查看防火墙状态
```bash
systemctl status firewalld.service
```
创建SSH免密登录
```bash
su - hadoop
```
```bash
ssh-keygen -t rsa -P ""
```
```bash
cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
```
```bash
chmod 700 ~/.ssh/authorized_keys
```
将公钥传输给slave1和slave2
```bash
scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/
```
```bash
scp ~/.ssh/authorized_keys hadoop@slave2:~/.ssh/
```
登录测试
```bash
ssh slave1
```
```bash
exit
```
```bash
ssh slave2
```
```bash
exit
```

### Slave1节点
查看IP地址
```bash
ip a
```
修改主机名为slave1
```bash
hostnamectl set-hostname slave1
```
```bash
bash
```
修改/etc/hosts文件，添加主机映射
```bash
vim /etc/hosts
```
添加如下内容
```
192.168.66.114 master
192.168.66.115 slave1
192.168.66.116 slave2
```
修改配置文件
```bash
vim /etc/sysconfig/ntpd
```
添加如下内容
```
YS_HWLOCK=yes
```
同步时间
```bash
systemctl start ntpd
```
```bash
date
```
关闭防火墙
```bash
systemctl stop firewalld.service
```
关闭防火墙自启
```bash
systemctl disable firewalld.service
```
查看防火墙状态
```bash
systemctl status firewalld.service
```
SSH免密登录相关操作（与master节点相同）
```bash
su - hadoop
```
```bash
ssh-keygen -t rsa -P ""
```
```bash
cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
```
```bash
chmod 700 ~/.ssh/authorized_keys
```

### Slave2节点
查看IP地址
```bash
ip a
```
修改主机名为slave2
```bash
hostnamectl set-hostname slave2
```
```bash
bash
```
修改/etc/hosts文件，添加主机映射
```bash
vim /etc/hosts
```
添加如下内容
```
192.168.66.114 master
192.168.66.115 slave1
192.168.66.116 slave2
```
修改配置文件
```bash
vim /etc/sysconfig/ntpd
```
添加如下内容
```
YS_HWLOCK=yes
```
同步时间
```bash
systemctl start ntpd
```
```bash
date
```
关闭防火墙
```bash
systemctl stop firewalld.service
```
关闭防火墙自启
```bash
systemctl disable firewalld.service
```
查看防火墙状态
```bash
systemctl status firewalld.service
```
SSH免密登录相关操作（与master节点相同）
```bash
su - hadoop
```
```bash
ssh-keygen -t rsa -P ""
```
```bash
cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
```
```bash
chmod 700 ~/.ssh/authorized_keys
```

## 实验二：Hadoop集群部署

### Master节点
解压安装Hadoop
```bash
su root
```
```bash
cd
```
```bash
tar -zxvf /opt/software/hadoop-2.7.1.tar.gz -C /usr/local/src/
```
```bash
mv /usr/local/src/hadoop-2.7.1 /usr/local/src/hadoop
```
配置Hadoop环境变量
```bash
vim /etc/profile
```
添加Hadoop环境变量
```
export HADOOP_HOME=/usr/local/src/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```
修改目录所有者和所有者组
```bash
chown -R hadoop:hadoop /usr/local/src
```
```bash
ll /usr/local/src/
```
解压安装jdk
```bash
tar -zxvf /opt/software/jdk-8u152-linux-x64.tar.gz -C /usr/local/src
```
更改 jdk 的名称
```bash
mv /usr/local/src/jdk1.8.0_152/ /usr/local/src/java
```
```bash
chown -R hadoop:hadoop /usr/local/src/java
```
配置java环境变量
```bash
vim /etc/profile
```
添加java环境变量
```
export JAVA_HOME=/usr/local/src/java
export PATH=$PATH:$JAVA_HOME/bin
```
生效环境变量
```bash
source /etc/profile
```
```bash
update-alternatives --install /usr/bin/java java /usr/local/src/java/bin/java 200
```
```bash
update-alternatives --set java /usr/local/src/java/bin/java
```
查看java和hadoop版本
```bash
java -version
```
```bash
hadoop version
```
配置Hadoop
```bash
cd /usr/local/src/hadoop/etc/hadoop
```
```bash
vim core-site.xml
```
在文件里添加如下配置
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/src/hadoop/tmp</value>
    </property>
</configuration>
```
```bash
vim hadoop-env.sh
```
在文件的最下方添加如下环境配置
```
export JAVA_HOME=/usr/local/src/java
export HADOOP_PERFIX=/usr/local/src/hadoop
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PERFIX/lib:$HADOOP_PERFIX/lib/natice"
```
```bash
vim hdfs-site.xml
```
在文件里添加如下配置
```xml
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/src/hadoop/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/src/hadoop/dfs/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
</configuration>
```
将副本拷贝成 mapred-queues.xml
```bash
cp mapred-site.xml.template mapred-site.xml
```
```bash
vim mapred-site.xml
```
在文件里添加如下配置
```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
    </property>
</configuration>
```
```bash
vim yarn-site.xml
```
在文件里添加如下配置
```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>master:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>master:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>master:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>master:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>master:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
</configuration>
```
修改 masters 配置文件
```bash
vim masters
```
加入以下配置信息
```
master
```
配置 slaves
```bash
vim slaves
```
在文件里改成如下配置
```
slave1
slave2
```
创建目录
```bash
mkdir -p /usr/local/src/hadoop/dfs/name
```
```bash
mkdir -p /usr/local/src/hadoop/dfs/data
```
```bash
mkdir -p /usr/local/src/hadoop/tmp
```
分发 hadoop 目录
```bash
scp -r /usr/local/src/hadoop/ root@slave1:/usr/local/src/
```
```bash
scp -r /usr/local/src/hadoop/ root@slave2:/usr/local/src/
```
```bash
scp -r /usr/local/src/java/ root@slave1:/usr/local/src/
```
```bash
scp -r /usr/local/src/java/ root@slave2:/usr/local/src/
```
分发环境配置
```bash
scp -r /etc/profile root@slave1:/etc/
```
```bash
scp -r /etc/profile root@slave2:/etc/
```
在每个节点上修改/usr/local/src/hadoop 目录的权限
### Master节点
```bash
chown -R hadoop:hadoop /usr/local/src/hadoop
```
### Slave1节点
```bash
chown -R hadoop:hadoop /usr/local/src/hadoop
```
生效环境配置
```bash
source /etc/profile
```
```bash
update-alternatives --install /usr/bin/java java /usr/local/src/java/bin/java 200
```
```bash
update-alternatives --set java /usr/local/src/java/bin/java
```
### Slave2节点
```bash
chown -R hadoop:hadoop /usr/local/src/hadoop
```
生效环境配置
```bash
source /etc/profile
```
```bash
update-alternatives --install /usr/bin/java java /usr/local/src/java/bin/java 200
```
```bash
update-alternatives --set java /usr/local/src/java/bin/java
```

## 实验三：Hadoop集群启动测试

### Master节点
格式化元数据
```bash
su hadoop
```
```bash
source /etc/profile
```
```bash
hdfs namenode -format
```

### Slave1节点
```bash
su hadoop
```
```bash
source /etc/profile
```

### Slave2节点
```bash
su hadoop
```
```bash
source /etc/profile
```

### Master节点
启动hdfs和yarn
```bash
start-dfs.sh
```
```bash
start-yarn.sh
```
查看进程
```bash
jps
```
查看Hadoop页面
<pre>
master:50070端口
master:8088端口
master:9000端口
</pre>

## Mapreduce测试
### Master节点
创建一个测试文件
```bash
vim a.txt
```
内容如下：
```
HELLO WORD
HELLO HADOOP
HELLO JAVA
```
在 hdfs 创建文件夹
```
hadoop fs -mkdir /input
```
将 a.txt 传输到 input 上
```bash
hadoop fs -put ~/a.txt /input
```
进入到 jar 包测试文件目录下
```bash
cd /usr/local/src/hadoop/share/hadoop/mapreduce/
```
测试 mapreduce
```bash
hadoop jar hadoop-mapreduce-examples-2.7.1.jar wordcount /input/a.txt /output
```
注：如果需要重复执行，需要删除输出目录，否则会报错
```bash
hdfs dfs -rm -r -f /output
```
查看 hdfs 下的传输结果
```bash
hadoop fs -lsr /output
```
查看文件测试的结果
```bash
hadoop fs -cat /output/part-r-00000
```
