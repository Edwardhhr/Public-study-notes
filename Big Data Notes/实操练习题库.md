## 基础环境配置

查看 ip
### root用户Master、Slave1、Slave2节点
```bash
ip a
```
修改主机名
### root用户Master
```bash
hostnamectl set-hostname master
bash
```
### root用户Slave1节点
```bash
hostnamectl set-hostname slave1
bash
```
### root用户Slave2节点
```bash
hostnamectl set-hostname slave2
bash
```
主机映射
### root用户Master、Slave1、Slave2节点
```bash
vim /etc/hosts
```
添加如下内容
```
192.168.66.114 master
192.168.66.115 slave1
192.168.66.116 slave2
```
时钟同步
```bash
vim /etc/sysconfig/ntpd
```
同步时间
```bash
systemctl start ntpd
date
```
关闭防火墙（三台都要关闭）
```bash
systemctl stop firewalld.service
```
关闭防火墙自启（三台都要关闭）
```bash
systemctl disable firewalld.service
```
查看防火墙状态
```bash
systemctl status firewalld.service
```
ssh 免密  
创建免密（三个主机同时进行）  
**root用户切换到hadoop用户**  
```bash
su - hadoop
```
```bash
ssh-keygen -t rsa -P ""
```
创建公钥
```bash
cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
```
给公钥执行权限
```bash
chmod 700 ~/.ssh/authorized_keys
```
Master节点将公钥传输给 slave1 和 slave2
### hadoop用户Master节点
```bash
scp ~/.ssh/authorized_keys hadoop@slave1:~/.ssh/
```
```bash
scp ~/.ssh/authorized_keys hadoop@slave2:~/.ssh/
```
登陆测试
```bash
ssh slave1
exit
ssh slave2
exit
```


## Hadoop 集群部署
```bash
su root
```
```bash
cd
```
解压安装 jdk
### root用户Master节点
```bash
tar -zxvf /opt/software/jdk-8u152-linux-x64.tar.gz -C /usr/local/src
```
更改 jdk 的名称
```bash
mv /usr/local/src/jdk1.8.0_152/ /usr/local/src/java
```
```bash
chown -R hadoop:hadoop /usr/local/src/java
```
配置 java 的环境变量
```bash
vim /etc/profile
```
配置如下环境
```
#java environment
export JAVA_HOME=/usr/local/src/java
export PATH=$PATH:$JAVA_HOME/bin
```
生效环境变量
```bash
source /etc/profile
```
查看 java 版本
```bash
java -version
```


## 安装 ZooKeeper
```bash
tar -zxvf /opt/software/zookeeper-3.4.8.tar.gz -C /usr/local/src/
```
```bash
cd /usr/local/src/
```
```bash
mv zookeeper-3.4.8 zookeeper
```
创建 ZooKeeper 数据目录
```bash
mkdir /usr/local/src/zookeeper/data
```
```bash
mkdir /usr/local/src/zookeeper/logs
```
配置 ZooKeeper 环境变量
```bash
cd ./zookeeper
```
```bash
vim /etc/profile
```
添加如下配置：
```
#zookeeper environment
export ZK_HOME=/usr/local/src/zookeeper
export PATH=$PATH:$ZK_HOME/bin
```
生效环境变量
```bash
source /etc/profile
```
修改 zoo.cfg 配置文件
```bash
cd conf/
```
<pre>
[root@master conf]# ls
configuration.xsl log4j.properties zoo_sample.cfg
</pre>
```bash
cp zoo_sample.cfg zoo.cfg
```
```bash
vim zoo.cfg
```
添加并更改如下配置：  
修改
```
dataDir=/usr/local/src/zookeeper/data
```
增加
```
dataLogDir=/usr/local/src/zookeeper/logs
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```
创建 myid 配置文件
```bash
cd ..
```
```bash
cd data/
```
```bash
echo "1" > myid
```


## ZooKeeper 集群启动  
分发 ZooKeeper 集群  
在这个目录下
<pre>
[root@master data]#
</pre>
```bash
scp -r /usr/local/src/zookeeper/ root@slave1:/usr/local/src/
```
```bash
scp -r /usr/local/src/zookeeper/ root@slave2:/usr/local/src/
```
分发环境变量并使其生效
```bash
scp /etc/profile root@slave1:/etc/
```
```bash
scp /etc/profile root@slave2:/etc/
```
```bash
source /etc/profile
```
### root用户Slave1、Slave2节点
```bash
source /etc/profile
```
修改 myid 配置  
master 对应 1,slave1 对应 2，slave2 对应 3
<pre>
[root@master data]# cat myid 
1
</pre>
### root用户Slave1节点
```bash
cd /usr/local/src/zookeeper/data/
```
```bash
echo "2" > myid
```
```bash
cat myid
```
### root用户Slave2节点
```bash
cd /usr/local/src/zookeeper/data/
```
```bash
echo "3" > myid
```
```bash
cat myid
```
修改 ZooKeeper 安装目录的归属用户为 hadoop 用户
### root用户Master、Slave1、Slave2节点
```bash
chown -R hadoop:hadoop /usr/local/src/zookeeper
```
启动 ZooKeeper 集群  
同时启动三个节点的 zookeeper
```bash
su hadoop
```
<pre>
[hadoop@master root]$ cd 
[hadoop@master ~]$ source /etc/profile
[hadoop@master ~]$ zkServer.sh start # ZooKeeper 启动
</pre>
```bash
cd
```
```bash
source /etc/profile
```
```bash
zkServer.sh start
```
查看状态
```bash
zkServer.sh status
```
<pre>
[hadoop@master ~]$ zkServer.sh status
JMX enabled by default
Using config: /usr/local/src/zookeeper/bin/ ../conf/zoo.cfg
Mode: follower # follower 状态
#slave1 节点状态
[hadoop@slave1 ~]$ zkServer.sh status
JMX enab1ed by default
Using config:' /usr/local/src/zookeeper/bin/ . ./conf/zoo.cfg
Mode:leader # leader 状态
#slave2 节点状态
[hadoop@slave2 ~]$ zkServer.sh status
JMX enabled by default
Using config:' /usr/local/src/zookeeper/bin/ . ./conf/zoo.cfg
Mode: follower # follower 状态
</pre>


## Hadoop HA 集群部署
**三个节点都进入hadoop用户**
### hadoop用户Master、Slave1、Slave2节点
```bash
su - hadoop
```
将 masterr 创建的公钥发给 slave1
### hadoop用户Master节点
```bash
scp ~/.ssh/authorized_keys root@slave1:~/.ssh/
```
将 slave1 的私钥加到公钥里
### hadoop用户Slave1节点
```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```
将公钥发给 slave2
```bash
scp ~/.ssh/authorized_keys root@slave2:~/.ssh/
```
登陆测试
<pre>
[hadoop@master ~]$ ssh slave1
Last login: Wed Jun 24 16:41:46 2020 from 192.168.1.7
[hadoop@slave1 ~]$ ssh slave2
Last login: Wed Jun 24 16:35:46 2020 from 192.168.1.1
[hadoop@slave2 ~]$ exit
登出
Connection to slave2 closed.
[hadoop@slave1 ~]$ exit
登出
Connection to slave1 closed.
</pre>
Hadoop HA 文件参数配置  
解压安装 Hadoop
### root用户master节点
```bash
su root
```
```bash
tar -zxvf /opt/software/hadoop-2.7.1.tar.gz -C /usr/local/src/
```
更改 hadoop 文件名
```bash
mv /usr/local/src/hadoop-2.7.1 /usr/local/src/hadoop
```
配置 hadoop 环境变量
```bash
vim /etc/profile
```
添加如下配置
```
#hadoop enviroment
export HADOOP_HOME=/usr/local/src/hadoop
export HADOOP_PREFIX=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_OPTS="-
Djava.library.path=$HADOOP_INSTALL/lib:$HADOOP_COMMON_LIB_NATIVE_DIR"
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```
配置 hadoop-env.sh 配置文件
```bash
cd /usr/local/src/hadoop/etc/hadoop
```
```bash
vim hadoop-env.sh
```
在最下面添加如下配置：
```
export JAVA_HOME=/usr/local/src/java
```
配置 core-site.xml 配置文件
```bash
vim core-site.xml
```
```xml
<!-- 指定 hdfs 的 nameservice 为 mycluster -->
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://mycluster</value>
</property>
<property>
    <name>hadoop.tmp.dir</name>
    <value>file:/usr/local/src/hadoop/tmp</value>
</property>
<!-- 指定 zookeeper 地址 -->
<property>
    <name>ha.zookeeper.quorum</name>
    <value>master:2181,slave1:2181,slave2:2181</value>
</property>
<!-- hadoop 链接 zookeeper 的超时时长设置 -->
<property>
    <name>ha.zookeeper.session-timeout.ms</name>
    <value>30000</value>
    <description>ms</description>
</property>
<property>
    <name>fs.trash.interval</name>
    <value>1440</value>
</property>
```
配置 hdfs-site.xml 配置文件
```bash
vim hdfs-site.xml
```
进行如下配置：
```xml
<!-- journalnode 集群之间通信的超时时间 -->
<property>
    <name>dfs.qjournal.start-segment.timeout.ms</name>
    <value>60000</value>
</property>
<!--指定 hdfs 的 nameservice 为 mycluster，需要和 core-site.xml 中的保持一致 
dfs.ha.namenodes.[nameservice id]为在 nameservice 中的每一个 NameNode 设置唯一标示
符。配置一个逗号分隔的NameNode ID列表。这将是被DataNode识别为所有的NameNode。
如果使用"mycluster"作为 nameservice ID，并且使用"master"和"slave1"作为 NameNodes 标
示符 -->
<property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
</property>
<!-- mycluster 下面有两个 NameNode，分别是 master，slave1 -->
<property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>master,slave1</value>
</property>
<!-- master 的 RPC 通信地址 -->
<property>
    <name>dfs.namenode.rpc-address.mycluster.master</name>
    <value>master:8020</value>
</property>
<!-- slave1 的 RPC 通信地址 -->
<property>
    <name>dfs.namenode.rpc-address.mycluster.slave1</name>
    <value>slave1:8020</value>
</property>
<!-- master 的 http 通信地址 -->
<property>
    <name>dfs.namenode.http-address.mycluster.master</name>
    <value>master:50070</value>
</property>
<!-- slave1 的 http 通信地址 -->
<property>
    <name>dfs.namenode.http-address.mycluster.slave1</name>
    <value>slave1:50070</value>
</property>
<!-- 指定 NameNode 的 edits 元数据的共享存储位置。也就是 JournalNode 列表
 该 url 的配置格式：qjournal://host1:port1;host2:port2;host3:port3/journalId
 journalId 推荐使用 nameservice，默认端口号是：8485 -->
<property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://master:8485;slave1:8485;slave2:8485/mycluster</value>
</property>
<!-- 配置失败自动切换实现方式 -->
<property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行 -->
<property>
    <name>dfs.ha.fencing.methods</name>
    <value>
 sshfence
 shell(/bin/true)
 </value>
</property>
<property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
</property>
<property>
    <name>dfs.support.append</name>
    <value>true</value>
</property>
<!-- 使用 sshfence 隔离机制时需要 ssh 免登陆 -->
<property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/root/.ssh/id_rsa</value>
</property>
<!-- 指定副本数 -->
<property>
    <name>dfs.replication</name>
    <value>2</value>
</property>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>/usr/local/src/hadoop/tmp/hdfs/nn</value>
</property>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>/usr/local/src/hadoop/tmp/hdfs/dn</value>
</property>
<!-- 指定 JournalNode 在本地磁盘存放数据的位置 -->
<property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/usr/local/src/hadoop/tmp/hdfs/jn</value>
</property>
<!-- 开启 NameNode 失败自动切换 -->
<property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
</property>
<!-- 启用 webhdfs -->
<property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
</property>
<!-- 配置 sshfence 隔离机制超时时间 -->
<property>
    <name>dfs.ha.fencing.ssh.connect-timeout</name>
    <value>30000</value>
</property>
<property>
    <name>ha.failover-controller.cli-check.rpc-timeout.ms</name>
    <value>60000</value>
</property>
```
配置 mapred-site.xml 配置文件
```bash
cp mapred-site.xml.template mapred-site.xml
```
```bash
vim mapred-site.xml
```
进行如下配置：
```xml
<!-- 指定 mr 框架为 yarn 方式 -->
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
<!-- 指定 mapreduce jobhistory 地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>master:10020</value>
</property>
<!-- 任务历史服务器的 web 地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>master:19888</value>
</property>
```
配置 yarn-site.xml 配置文件
```bash
yarn-site.xml
```
进行如下配置：
```xml
<!-- Site specific YARN configuration properties -->
<!-- 开启 RM 高可用 -->
<property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
</property>
<!-- 指定 RM 的 cluster id -->
<property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>yrc</value>
</property>
<!-- 指定 RM 的名字 -->
<property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2</value>
</property>
<!-- 分别指定 RM 的地址 -->
<property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>master</value>
</property>
<property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>slave1</value>
</property>
<!-- 指定 zk 集群地址 -->
<property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>master:2181,slave1:2181,slave2:2181</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>86400</value>
</property>
<!-- 启用自动恢复 -->
<property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
</property>
<!-- 制定 resourcemanager 的状态信息存储在 zookeeper 集群上 -->
<property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>
```
配置 slaves 配置文件
```bash
vim slaves
```
进行如下配置：
```
master
slave1
slave2
```
解压包到指定目录
```bash
mkdir -p /usr/local/src/hadoop/tmp/hdfs/nn
mkdir -p /usr/local/src/hadoop/tmp/hdfs/dn
mkdir -p /usr/local/src/hadoop/tmp/hdfs/jn
mkdir -p /usr/local/src/hadoop/tmp/logs
```
分发文件
```bash
scp -r /etc/profile root@slave1:/etc/
scp -r /etc/profile root@slave2:/etc/
scp -r /usr/local/src/hadoop root@slave1:/usr/local/src/
scp -r /usr/local/src/hadoop root@slave2:/usr/local/src/
```
修改目录所有者和所有者组
### root用户master、slave1、slave2节点
要将目录/usr/local/src 的所有者改为 hadoop 用户
```bash
chown -R hadoop:hadoop /usr/local/src/hadoop/
```
生效环境变量
```bash
su hadoop
```
```bash
source /etc/profile
```
<pre>
[root@master hadoop]# su hadoop
[hadoop@master hadoop]$ source /etc/profile
[root@slave1 hadoop]# su hadoop
[hadoop@slave1 hadoop]$ source /etc/profile
[root@slave2 hadoop]# su hadoop
[hadoop@slave2 hadoop]$ source /etc/profile
</pre>

## 高可用集群启动
启动 journalnode 守护进程
### hadoop用户master节点
```bash
hadoop-daemons.sh start journalnode
```
初始化 namenode
```bash
hdfs namenode -format
```
注册 ZNode
```bash
hdfs zkfc -formatZK
```
启动 hdfs
```bash
start-dfs.sh
```
启动 yarn
```bash
start-yarn.sh
```
同步 master 数据  
复制 namenode 元数据到其它节点(在 master 节点执行)
```bash
scp -r /usr/local/src/hadoop/tmp/hdfs/nn/* slave1:/usr/local/src/hadoop/tmp/hdfs/nn/
scp -r /usr/local/src/hadoop/tmp/hdfs/nn/* slave2:/usr/local/src/hadoop/tmp/hdfs/nn/
```
在 slave1 上启动 resourcemanager 和 namenode 进程
### hadoop用户slave1节点
```bash
yarn-daemon.sh start resourcemanager
```
```bash
hadoop-daemon.sh start namenode
```
启动 MapReduce 任务历史服务器
### hadoop用户master节点
```bash
yarn-daemon.sh start proxyserver
```
```bash
mr-jobhistory-daemon.sh start historyserver
```
### hadoop用户master、slave1、slave2节点
查看端口和进程
```bash
jps
```
浏览器中打开
```bash
master:50070
```
```bash
slave1:50070
```
```bash
master:8088
```

## ZooKeeper 组件部署
集群模式部署 ZooKeeper(已安装无需安装)
<pre>
[root@master ~]# tar -zxvf /opt/software/zookeeper-3.4.8.tar.gz -C /usr/local/src/
</pre>
### root用户master节点
```bash
mv /usr/local/src/zookeeper-3.4.8/ /usr/local/src/zookeeper
```
修改配置文件
```bash
cd /usr/local/src/zookeeper/conf/
```
```bash
mv zoo_sample.cfg /usr/local/src/zookeeper/conf/zoo.cfg
```
```bash
vim /usr/local/src/zookeeper/conf/zoo.cfg
```
完整配置内容如下：
```
tickTime=2000
initLimit=10
syncLimit=5
clientPort=2181
dataDir=/usr/local/src/zookeeper/data
dataLogDir=/usr/local/src/zookeeper/logs
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```
```bash
mkdir -p /usr/local/src/zookeeper/data
```
```bash
mkdir -p /usr/local/src/zookeeper/logs
```
```bash
vim /usr/local/src/zookeeper/data/myid
```
以 master 节点为例，文件中只写入一个数字：1
```
1
```
配置 ZooKeeper 环境变量
```bash
vim /etc/profile
```
添加如下配置：
```
#zookeeper environment
export ZK_HOME=/usr/local/src/zookeeper
export PATH=$PATH:$ZK_HOME/bin
```
文件分发  
将 master 主节点已经配置好 ZooKeepr 文件分发给集群从节点
```bash
scp -r /usr/local/src/zookeeper root@slave1:/usr/local/src/
```
```bash
scp -r /usr/local/src/zookeeper root@slave2:/usr/local/src/
```
```bash
scp /etc/profile root@slave1:/etc/
```
```bash
scp /etc/profile root@slave2:/etc/
```
在分发的从节点上需要对每一个 myid 文件进行修改，如 slave1从节点修改为:2。与 zoo.cfg 配置文件相对应  
### root用户Slave1节点
```bash
vim /usr/local/src/zookeeper/data/myid
```
```
2
```
### root用户Slave2节点
```bash
vim /usr/local/src/zookeeper/data/myid
```
```
3
```
修改 ZooKeeper 安装目录的归属用户为 hadoop 用户
### root用户master、slave1、slave2节点
```bash
chown -R hadoop:hadoop /usr/local/src/zookeeper
```
关闭防火墙
```bash
systemctl stop firewalld.service
```
关闭防火墙自启
```bash
systemctl disable firewalld.service
```
启动
```bash
su - hadoop
```
```bash
source /etc/profile
```
```bash
./zkServer.sh start
```
查看状态
```bash
./zkServer.sh status
```


## Kafka 组件部署
将 kafka 的 tar.gz 安装包解压
### root用户master节点
```bash
cd /usr/local/src
```
```bash
tar -zxvf /opt/software/kafka1.0.0.tgz -C /usr/local/src/
```
将解压好的 kafka 文件重命名
```bash
mv kafka_2.11-1.0.0/ kafka
```
修改 server.properties
```bash
vim /usr/local/src/kafka/config/server.properties
```
在 server.properties 文件下找到下列配置项，并修改为：
```
broker.id=0
zookeeper.connect=master,slave1,slave2
```
使用 scp 命令把 kafka 发送到各个节点
```bash
scp -r /usr/local/src/kafka/ root@slave1:/usr/local/src/kafka/
scp -r /usr/local/src/kafka/ root@slave2:/usr/local/src/kafka/
```
### root用户master、slave1、slave2节点
```bash
chown -R hadoop:hadoop /usr/local/src/kafka
```
修改各个节点中的 server.properties
### root用户slave1节点
```bash
su hadoop
```
### hadoop用户slave1节点
```bash
vim /usr/local/src/kafka/config/server.properties
```
在 server.properties 文件下找到 broker.id 修改为  
**Slave1**：
```
broker.id=1
```
### root用户slave2节点
```bash
su hadoop
```
### hadoop用户slave2节点
```bash
vim /usr/local/src/kafka/config/server.properties
```
在 server.properties 文件下找到 broker.id 修改为  
**Slave2**：
```
broker.id=2
```
启动 ZOOKEEPER 集群  
在各节点启动 zookeeper
### hadoop用户master、slave1、slave2节点
进入家目录
```bash
cd
```
启动 zookeeper
```bash
zkServer.sh start
```
在各个节点启动 kafka 服务
```bash
/usr/local/src/kafka/bin/kafka-server-start.sh /usr/local/src/kafka/config/server.properties
```

## Kafka 组件验证部署
前提需要安装 Zookeeper 组件  
创建一个名为 hello 的 topic
### hadoop用户master节点
```bash
/usr/local/src/kafka/bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --topic hello --partitions 1
```
查看 topic 是否创建成功
```bash
/usr/local/src/kafka/bin/kafka-topics.sh --list --zookeeper master:2181,slave1:2181,slave2:2181
```
在 master 节点中创建一个生产者
```bash
/usr/local/src/kafka/bin/kafka-console-producer.sh --broker-list master:9092,slave1:9092,slave2:9092 --topic hello
```
在 slave1 节点中创建一个消费者
### hadoop用户slave1节点
```bash
/usr/local/src/kafka/bin/kafka-console-consumer.sh --zookeeper master:2181,slave1:2181,slave2:2181 --topic hello --from-beginning
```
在创建的生产者中输入信息
<pre>
>hello kafka
</pre>
在创建的消费者中查看信息  
接受成功，则 kafka 组件验证成功  
在创建消费者的终端中可以看到以下输出信息
<pre>
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future 
major release. Consider using the new consumer by passing [bootstrap-server] instead of 
[zookeeper].
hello kafka
</pre>


## Scala 的安装
解压 Scala 压缩文件并重命名
### root用户master节点
```bash
tar -zxvf /opt/software/scala-2.11.8.tgz -C /usr/local/src/
```
```bash
mv /usr/local/src/scala-2.11.8/ /usr/local/src/scala
```
分发scala到子节点
```bash
scp -r /usr/local/src/scala/ root@slave1:/usr/local/src/
```
```bash
scp -r /usr/local/src/scala/ root@slave2:/usr/local/src/
```
修改 scala 目录的用户权限
```bash
chown -R hadoop:hadoop /usr/local/src/scala
```
配置环境变量
```bash
vim /etc/profile
```
添加如下内容
```
export SCALA_HOME=/usr/local/src/scala
export PATH=$PATH:$SCALA_HOME/bin
```
使环境变量生效
```bash
su hadoop
```
```bash
source /etc/profile
```
验证 Scala 安装是否成功
### hadoop用户master节点
```bash
cd /usr/local/scr/scala/bin
```
输入 scala 进入 scala shell 交互编程界面：
```bash
scala
```
退出 scala shell 的命令为：
```bash
quit
```


## 安装 Spark
首先要安装Scala  
在 master 节点上安装 spark
### root用户master节点
```bash
su root
```
```bash
cd
```
将 Spark 包解压到/usr/local/src 下
```bash
tar -zxvf /opt/software/spark-2.0.0-bin-hadoop2.6.tgz -C /usr/local/src/
```
Spark 解压后的重命名操作
```bash
mv /usr/local/src/spark-2.0.0-bin-hadoop2.6/ /usr/local/src/spark
```
修改 spark 目录的用户权限
```bash
chown -R hadoop:hadoop /usr/local/src/spark
```
配置环境变量
```bash
vim /etc/profile
```
在文件中加入如下内容：
```
export SPARK_HOME=/usr/local/src/spark
export PATH=$PATH:$SPARK_HOME/bin:$PATH
```
### hadoop用户master节点
```bash
su hadoop
```
生效环境变量
```bash
source /etc/profile
```
修改 Spark 参数  
进入 Spark 的配置文件目录“conf”：
```bash
cd /usr/local/src/spark/conf/
```
将已有的文件“spark-env.sh.template”复制出来并命名为 spark-evn.sh：
```bash
cp /usr/local/src/spark/conf/spark-env.sh.template /usr/local/src/spark/conf/spark-env.sh
```
进入 spark 配置文件“spark-env.sh”，命令为：
```bash
vim /usr/local/src/spark/conf/spark-env.sh
```
然后将下面所示内容加入到文件`spark-env.sh`中：
```
export JAVA_HOME=/usr/local/src/java
export HADOOP_HOME=/usr/local/src/hadoop
export SCALA_HOME=/usr/local/src/scala
export SPARK_MASTER_IP=master
export SPARK_MASTER_PORT=7077
export SPARK_DIST_CLASSPATH=$(/usr/local/src/hadoop/bin/hadoop classpath)
export HADOOP_CONF_DIR=/usr/local/src/hadoop/etc/hadoop
export SPARK_YARN_USER_ENV="CLASSPATH=/usr/local/src/hadoop/etc/hadoop"
export YARN_CONF_DIR=/usr/local/src/hadoop/etc/hadoop
```
配置 slaves 文件
```bash
cp /usr/local/src/spark/conf/slaves.template /usr/local/src/spark/conf/slaves
```
```bash
vim slaves
```
将文件中的内容修改为以下内容：
```
master
slave1
slave2
```
在两个 slaves 从节点上安装 Spark  
将 master 主节点上的 Spark 安装目录和.bashrc 环境变量复制到两个 slaves 从节点上  
### root用户master节点
```bash
su root
```
```bash
scp -r /usr/local/src/spark/ root@slave1:/usr/local/src/
```
```bash
scp -r /usr/local/src/spark/ root@slave2:/usr/local/src/
```
```bash
scp /etc/profile root@slave1:/etc/
```
```bash
scp /etc/profile root@slave2:/etc/
```
在 slave1、slave2 节点上分别安装 Spark
### root用户slave1、slave2节点
```bash
chown -R hadoop:hadoop /usr/local/src/spark/
```
```bash
su hadoop
```
```bash
source /etc/profile
```
启动 Hadoop 集群
### hadoop用户master、slave1、slave2节点
```bash
su hadoop
```
```bash
cd /usr/local/src/zookeeper/bin/
```
```bash
./zkServer.sh start
```
在 master 节点启动 hadoop 集群
### hadoop用户master节点
```bash
cd /usr/local/src/hadoop/sbin/
```
```bash
./start-all.sh
```
以集群模式运行 SparkPi 实例程序
```bash
cd /usr/local/src/spark/
```
```bash
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode client --driver-memory 512M --executor-memory 512M --executor-cores 1 examples/jars/spark-examples_2.11-2.0.0.jar 40
```
在运行结果中间可以找到我们需要的 pi 值，如下所示：
<pre>
20/07/04 05:48:48 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, 
took 3.866892 s
Pi is roughly 3.141053785263446
</pre>
在 master 节点上打开浏览器，访问 `http://master:8088` 显示 yarn 的信息就可以看到我们运行的 SparkPi 实例程序
